{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import gym\n",
    "import neurogym as ngym\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "task = 'ContextDecisionMaking-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to define save path of model\n",
    "def get_modelpath(task):\n",
    "    # Make a local file directories\n",
    "    path = Path('.') / 'files'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    path = path / task\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "modelpath = get_modelpath(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class PosWLinear(nn.Module):\n",
    "    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
    "\n",
    "    Same as nn.Linear, except that weight matrix is constrained to be non-negative\n",
    "    \"\"\"\n",
    "    __constants__ = ['bias', 'in_features', 'out_features']\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(PosWLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # weight is non-negative\n",
    "        return F.linear(input, torch.abs(self.weight), self.bias)\n",
    "    \n",
    "    \n",
    "class EIRecLinear(nn.Module):\n",
    "    r\"\"\"Recurrent E-I Linear transformation.\n",
    "    \n",
    "    Args:\n",
    "        hidden_size: int, layer size\n",
    "        e_prop: float between 0 and 1, proportion of excitatory units\n",
    "    \"\"\"\n",
    "    __constants__ = ['bias', 'hidden_size', 'e_prop']\n",
    "\n",
    "    def __init__(self, hidden_size, e_prop, bias=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.e_prop = e_prop\n",
    "        self.e_size = int(e_prop * hidden_size)\n",
    "        self.i_size = hidden_size - self.e_size\n",
    "        self.weight = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        mask = np.tile([1]*self.e_size+[-1]*self.i_size, (hidden_size, 1))\n",
    "        np.fill_diagonal(mask, 0)\n",
    "        self.mask = torch.tensor(mask, dtype=torch.float32)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        # Scale E weight by E-I ratio\n",
    "        self.weight.data[:, :self.e_size] /= (self.e_size/self.i_size)\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "    def effective_weight(self):\n",
    "        return torch.abs(self.weight) * self.mask\n",
    "\n",
    "    def forward(self, input):\n",
    "        # weight is non-negative\n",
    "        return F.linear(input, self.effective_weight(), self.bias)\n",
    "\n",
    "\n",
    "class EIRNN(nn.Module):\n",
    "    \"\"\"E-I RNN.\n",
    "    \n",
    "    Reference:\n",
    "        Song, H.F., Yang, G.R. and Wang, X.J., 2016.\n",
    "        Training excitatory-inhibitory recurrent neural networks\n",
    "        for cognitive tasks: a simple and flexible framework.\n",
    "        PLoS computational biology, 12(2).\n",
    "\n",
    "    Args:\n",
    "        input_size: Number of input neurons\n",
    "        hidden_size: Number of hidden neurons\n",
    "\n",
    "    Inputs:\n",
    "        input: (seq_len, batch, input_size)\n",
    "        hidden: (batch, hidden_size)\n",
    "        e_prop: float between 0 and 1, proportion of excitatory neurons\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, dt=None,\n",
    "                 e_prop=0.8, sigma_rec=0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.e_size = int(hidden_size * e_prop)\n",
    "        self.i_size = hidden_size - self.e_size\n",
    "        self.num_layers = 1\n",
    "        self.tau = 100\n",
    "        if dt is None:\n",
    "            alpha = 1\n",
    "        else:\n",
    "            alpha = dt / self.tau\n",
    "        self.alpha = alpha\n",
    "        self.oneminusalpha = 1 - alpha\n",
    "        # Recurrent noise\n",
    "        self._sigma_rec = np.sqrt(2*alpha) * sigma_rec\n",
    "\n",
    "        # self.input2h = PosWLinear(input_size, hidden_size)\n",
    "        self.input2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2h = EIRecLinear(hidden_size, e_prop=0.8)\n",
    "\n",
    "    def init_hidden(self, input):\n",
    "        batch_size = input.shape[1]\n",
    "        return (torch.zeros(batch_size, self.hidden_size).to(input.device),\n",
    "                torch.zeros(batch_size, self.hidden_size).to(input.device))\n",
    "\n",
    "    def recurrence(self, input, hidden):\n",
    "        \"\"\"Recurrence helper.\"\"\"\n",
    "        state, output = hidden\n",
    "        total_input = self.input2h(input) + self.h2h(output)\n",
    "        state = state * self.oneminusalpha + total_input * self.alpha\n",
    "        state += self._sigma_rec * torch.randn_like(state)\n",
    "        output = torch.relu(state)\n",
    "        return state, output\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        \"\"\"Propogate input through the network.\"\"\"\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(input)\n",
    "\n",
    "        output = []\n",
    "        steps = range(input.size(0))\n",
    "        for i in steps:\n",
    "            hidden = self.recurrence(input[i], hidden)\n",
    "            output.append(hidden[1])\n",
    "\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"Recurrent network model.\n",
    "\n",
    "    Args:\n",
    "        input_size: int, input size\n",
    "        hidden_size: int, hidden size\n",
    "        output_size: int, output size\n",
    "        rnn: str, type of RNN, lstm, rnn, ctrnn, or eirnn\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Excitatory-inhibitory RNN\n",
    "        self.rnn = EIRNN(input_size, hidden_size, **kwargs)\n",
    "        # self.fc = PosWLinear(self.rnn.e_size, output_size)\n",
    "        self.fc = nn.Linear(self.rnn.e_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        rnn_activity, _ = self.rnn(x)\n",
    "        rnn_e = rnn_activity[:, :, :self.rnn.e_size]\n",
    "        out = self.fc(rnn_e)\n",
    "        return out, rnn_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-familiar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network Configs \n",
    "\n",
    "config = {\n",
    "    'dt': 100,\n",
    "    'hidden_size': 128,\n",
    "    'lr': .01,\n",
    "    'batch_size': 16,\n",
    "    'seq_len': 100,\n",
    "    'envid': task,\n",
    "}\n",
    "\n",
    "timing = {\n",
    "            'fixation': 300,\n",
    "            # 'target': 350,\n",
    "            'stimulus': 750,\n",
    "            'delay': ngym.random.TruncExp(600, 300, 3000),\n",
    "            'decision': 100\n",
    "}\n",
    "\n",
    "seq_len = 100\n",
    "kwargs = {'dt': 100, 'timing': timing}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-victorian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make training data set \n",
    "\n",
    "# Save config\n",
    "with open(modelpath / 'config.json', 'w') as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "# Make supervised dataset\n",
    "dataset = ngym.Dataset(task, env_kwargs=kwargs, batch_size=16,\n",
    "                       seq_len=seq_len)\n",
    "\n",
    "# A sample environment from dataset\n",
    "env = dataset.env\n",
    "\n",
    "# Network input and output size\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Network \n",
    "\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Instantiate the network and print information\n",
    "hidden_size = 128\n",
    "net = Net(input_size=input_size, hidden_size=hidden_size,\n",
    "          output_size=output_size, dt=env.dt, sigma_rec=0.15)\n",
    "print(net)\n",
    "\n",
    "# Use Adam optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "running_loss = 0\n",
    "running_acc = 0\n",
    "print_step = 500\n",
    "\n",
    "print('Training task ', task)\n",
    "\n",
    "for i in range(5000):\n",
    "    inputs, labels = dataset()\n",
    "    inputs = torch.from_numpy(inputs).type(torch.float)\n",
    "    labels = torch.from_numpy(labels.flatten()).type(torch.long)\n",
    "\n",
    "    # in your training loop:\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output, activity = net(inputs)\n",
    "    \n",
    "    output = output.view(-1, output_size)\n",
    "    loss = criterion(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "\n",
    "    running_loss += loss.item()\n",
    "    if i % print_step == (print_step - 1):\n",
    "        running_loss /= print_step\n",
    "        print('Step {}, Loss {:0.4f}'.format(i+1, running_loss))\n",
    "        torch.save(net.state_dict(), modelpath / 'net.pth')\n",
    "        running_loss = 0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_test_timing(env):\n",
    "    \"\"\"Infer timing of environment for testing.\"\"\"\n",
    "    timing = {}\n",
    "    for period in env.timing.keys():\n",
    "        period_times = [env.sample_time(period) for _ in range(100)]\n",
    "        timing[period] = np.median(period_times)\n",
    "    return timing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
